{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediciting High School Graduation Rate Based on Holistic Data of Previous Cohorts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents:\n",
    "- [Project Overview](#Project-Overview)\n",
    "- [Creating an environment from the environment.yml file](#Creating-an-environment-from-the-environment.yml-file)\n",
    "- [Data Source](#Data-Source)\n",
    "- [Data Cleaning](#Data-Cleaning)\n",
    "- [Exploratory Data Analysis](#Exploratory-Data-Analysis)\n",
    "- [First Simple Model](#First-Simple-Model)\n",
    "- [Model Selection](#Model-Selection) \n",
    "- [Final Model Evaluation](#Final-Model-Evaluation)\n",
    "- [Conclusion](#Conclusion)\n",
    "- [Future Improvement Ideas](#Future-Improvement-Ideas)\n",
    "- [Contact](#Contact)\n",
    "- [Presentation Slide Deck](#[Presentation-Slide-Deck])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "\n",
    "The goal of this project is to create a model that can predict the 4-year graduation rate of a high school cohort based on data regarding the broader structure of the instructional setting and the experience of previous cohorts.\n",
    "\n",
    "If it is possible to make such a prediction using readily available information it could be possible to spot performance trends, identify cohorts in need of additional resources, or for school to project target goals for improved academic outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an environment from the environment.yml file\n",
    "To run the code in these notebook, use the terminal or an Anaconda Prompt for the following steps:\n",
    "\n",
    "Create the environment from the [environment.yml](https://github.com/dannmorr/predicting-ibse-graduation-rate/blob/master/environment.yml) file:\n",
    "`conda env create -f environment.yml`\n",
    "\n",
    "The first line of the yml file sets the new environment's name. \n",
    "\n",
    "Activate the new environment: \n",
    "`conda activate myenv`\n",
    "\n",
    "Verify that the new environment was installed correctly:\n",
    "`conda env list`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Source\n",
    "Data was gathered from the Illinois State Board of Education's [Illinois Report Card Data Library](https://www.isbe.net/pages/illinois-state-report-card-data.aspx) website. \n",
    "From the website: \n",
    ">The Report Card Data Library page is the repository for Report Card data available for public use. Here you can find Statewide Trend Data, Report Card Glossary of Terms, and the public data files from which the Report Card is produced annually.\n",
    "\n",
    "The data for academic years 2018 and 2019 are each available to download in a single .xlsx Data File.\n",
    "\n",
    "Gathering the data for each of the academic years 2013 - 2017 involves downloading semi-colon separate .txt Data Files and an accompanying Layout File.\n",
    "\n",
    "Links for all of the downloads are available in the table below.\n",
    "\n",
    "Cleaned .xlsx versions of the Data Files are contained in the [data folder](https://github.com/dannmorr/predicting-ibse-graduation-rate/tree/master/data) of this repo. \n",
    "\n",
    "Copies of the Layout Files are also available in the [references folder](https://github.com/dannmorr/predicting-ibse-graduation-rate/blob/master/references/Open_Illinois_Report_Card_Data_Files.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Year | Link to Data File                                                                                      | Link to Layout File                                                               |\n",
    "|------|--------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------|\n",
    "| 2019 | https://www.isbe.net/_layouts/Download.aspx?SourceUrl=/Documents/2019-Report-Card-Public-Data-Set.xlsx | n/a                                                                               |\n",
    "| 2018 | https://www.isbe.net/_layouts/Download.aspx?SourceUrl=/Documents/Report-Card-Public-Data-Set.xlsx      | n/a                                                                               |\n",
    "| 2017 | https://www.isbe.net/Documents/rc17.zip                                                                | https://www.isbe.net/_layouts/Download.aspx?SourceUrl=/Documents/RC17_layout.xlsx |\n",
    "| 2016 | https://www.isbe.net/_layouts/Download.aspx?SourceUrl=/Documents/rc16.zip                              | https://www.isbe.net/_layouts/Download.aspx?SourceUrl=/Documents/RC16-layout.xlsx |\n",
    "| 2015 | https://www.isbe.net/_layouts/Download.aspx?SourceUrl=/Documents/rc15.zip                              | https://www.isbe.net/_layouts/Download.aspx?SourceUrl=/Documents/RC15-layout.xlsx |\n",
    "| 2014 | https://www.isbe.net/_layouts/Download.aspx?SourceUrl=/Documents/rc14.zip                              | https://www.isbe.net/_layouts/Download.aspx?SourceUrl=/Documents/RC14_layout.xlsx |\n",
    "| 2013 | https://www.isbe.net/_layouts/Download.aspx?SourceUrl=/Documents/2013-rc-separated.zip                 | https://www.isbe.net/_layouts/Download.aspx?SourceUrl=/Documents/RC13_layout.xlsx |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions for Opening the Report Card Data Files is available in the [references folder](https://github.com/dannmorr/predicting-ibse-graduation-rate/blob/master/references/Open_Illinois_Report_Card_Data_Files.pdf) or can be downloaded [here](https://www.isbe.net/Documents/Open_Illinois_Report_Card_Data_Files.pdf).\n",
    "\n",
    ">\"The Illinois Report Card data files that are available for download from ISBEâ€™s website do not include a header row. You must refer to the companion report card file layout Excel document to understandhow the data is organized within the worksheet.\"\n",
    "\n",
    "While performing the tasks of downloading the files, and reviewing the contents and Layout Files, I also began to narrow down my selected feature set to use for this project. From the Data Files, I identified 35 features across these 6 categories that I felt would give an adequate holistic view on the students' experience:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. General School Information**\n",
    " - school_name\n",
    " - district\n",
    " - city\n",
    " - county\n",
    " - district_type\n",
    " - district_size\n",
    " - school_type\n",
    " - grades_served\n",
    " \n",
    "**2. Student Demographics**\n",
    " - percent_student_enrollment_white\n",
    " - percent_student_enrollment_black_or_african_american\n",
    " - percent_student_enrollment_hispanic_or_latino\n",
    " - percent_student_enrollment_asian\n",
    " - percent_student_enrollment_native_hawaiian_or_other_pacific_islander\n",
    " - percent_student_enrollment_american_indian_or_alaska_native\n",
    " - percent_student_enrollment_two_or_more_races\n",
    " - number_student_enrollment\n",
    " - total_number_of_school_days\n",
    " - student_attendance_rate\n",
    " - student_chronic_truancy_rate\n",
    " - high_school_dropout_rate_total\n",
    " - high_school_4_year_graduation_rate_total\n",
    " - high_school_5_year_graduation_rate_total\n",
    " \n",
    "**3. Instructional Setting**\n",
    " - avg_class_size_high_school\n",
    " \n",
    "**4. Teacher and Administrator Statistics**\n",
    " - pupil_teacher_ratio_high_school\n",
    " - teacher_avg_salary\n",
    " - teacher_retention_rate\n",
    " - principal_turnover_within_6_years\n",
    " \n",
    "**5. College and Career Readiness**\n",
    " - percent_graduates_enrolled_in_a_postsecondary_institution_within_16_months\n",
    " - percent_graduates_enrolled_in_a_postsecondary_institution_within_12_months\n",
    " - percent_9th_grade_on_track\n",
    " \n",
    "**6. Advanced Coursework**\n",
    " - number_students_who_took_ap_classes_grade_10_total\n",
    " - number_students_who_took_ap_classes_grade_11_total\n",
    " - number_students_who_took_ap_classes_grade_12_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in the data sets represents one school in a given academic year. These data sets include all public and charter schools in Illinois serving grades PreK - 12.\n",
    "\n",
    "The number of columns varied greatly from set to set (from 800 on the low end to 9,000 on the high end), as did the parameters that were recorded from one year to the next. The features listed above were consistenly available throughout these data files.\n",
    "\n",
    "I will use the following cohorts for training the data:\n",
    "\n",
    "- 2017\n",
    "- 2016\n",
    "- 2015\n",
    "- 2014\n",
    "- 2013\n",
    "\n",
    "My validation set will be: \n",
    "- 2018\n",
    "\n",
    "My test set will be \n",
    "- 2019\n",
    "\n",
    "The target variable is **4-year high school graduation rate**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "The import cell contains the libraries needed to run the code in this repo, and also includes plot parameters for charts, and functions contained in a .py file in the src folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "import seaborn as sns \n",
    "\n",
    "# statsmodels imports\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import r2_score, SCORERS\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# turn off warnings\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', category = DeprecationWarning)\n",
    "warnings.simplefilter('ignore', category = FutureWarning)\n",
    "\n",
    "# plot parameters\n",
    "plt.rcParams['figure.figsize'] = 12, 12\n",
    "plt.rcParams['axes.labelsize'] = 20\n",
    "plt.rcParams['axes.titlesize'] = 25\n",
    "plt.rcParams['xtick.labelsize'] = 18\n",
    "plt.rcParams['ytick.labelsize'] = 18\n",
    "plt.rcParams['axes.edgecolor'] = 'black'\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "plt.rcParams['font.size'] = 16\n",
    "\n",
    "# functions from .py file\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join(os.pardir, os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import src.eda_functions as fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "This is a brief overview of the steps taken to clean the data and combine the files into one working DataFrame.\n",
    "The details can be seen in [notebooks/exploratory/01_cleaning_and_compiling](https://github.com/dannmorr/predicting-ibse-graduation-rate/blob/master/notebooks/exploratory/01_cleaning_and_compiling.ipynb)\n",
    "\n",
    "1. Read in each cohort data file and create a dataframe for schools that serve grades 9 - 12.\n",
    "\n",
    "    I chose this parameter because there are some charter schools, and schools in smaller districts that serve more grades than 9-12 (and a couple that serve 10-12 or just 11-12. \n",
    "\n",
    "    If they serve through grade 12, they submit graduation rate information. If they do not serve grade 12, they do not contain my target variable.\n",
    "    \n",
    "2. Convert numeric columns from 'object' to 'float'. Many of the columns had numeric values that were recorded as strings. These were converted to numbers. In some cases this revealed missing values that were replaced with zeros (see next step), others were later imputed with mean values - after performing a train test split. \n",
    "\n",
    "3. Fill in NaNs with zeros as appropriate:\n",
    "    For example, looking at percentages of student demographics, all groups may not be represented and, therefore, cells left blank instead of recording a zero.\n",
    "    Similarly, not all schools offer Advanced Placement classes for grades 10, 11 and 12. Where these are blank, they have been recorded as zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The individual DataFrames were concatonated and saved as `merged_df`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df contains all cohorts 2013 - 2019\n",
    "merged_df = pd.read_csv('../../data/merged_df.csv')\n",
    "fun.to_obj(merged_df, 'cohort')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function runs a nice overview of the DateFrame to look for missing values, dtypes, mean, median, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fun.proj_eda(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged_df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Starting from the largest values:\n",
    "- **Avg teacher salary will be dropped.** \n",
    "\n",
    "The data is reported at the disctrict level, not by every school. Also I've reconsidered the usefulness of that item.\n",
    "- **Pupil-Teacher ratio will be dropped.** \n",
    "\n",
    "It was reported by school for 2013-2017, but only at a district level for 2018 and 2019. I don't want to mix the two at this time\n",
    "- **AP classes will be dropped.**\n",
    "\n",
    "I feel like this is important. It wasn't entered for 2013 or 2014, which is a lot of missing values. Many schools don't offer AP classes, so it may also act as a penalty to them. Since they are more than 50% NaNs (or zeros), I should drop them. \n",
    "- **Percent of Graduates columns. Further investigating at the ISBE Data Library showed that these NaNs should be recorded as zeros.**\n",
    "\n",
    "- **Percent of 9th graders. Further investigating at the ISBE Data Library showed that these NaNs should have been recorded as zeros.**\n",
    "\n",
    "- **Teacher retention rate. I will replace with mean of the column when imputing.**\n",
    "\n",
    "- **Principal turnover will be dropped.** \n",
    "\n",
    "Like teacher salary, I'm not sure how important it really is.\n",
    "- **5 year graduation rate. I will replace with mean of the column when imputing.**\n",
    "\n",
    "- **Avg class size. I will replace with mean of the column when imputing.**\n",
    "\n",
    "- **Dropout rate. I will replace with mean of the column when imputing.**\n",
    "\n",
    "\n",
    "- **4 year graduation rate.** \n",
    "\n",
    "Since it was a small number missing in each year, I returned to the datasets, and ISBE website to research each school and can confirm that these nans should be zeros. These are primarily small class sizes and/or schools with struggling graduation rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After dropping the necessary columns, I performed my Train/Validation/Test split on the merged DataFrame.\n",
    "As mentioned above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set contains cohorts 2013 - 2017\n",
    "train_set = pd.read_csv('../../data/train_set.csv')\n",
    "fun.to_obj(train_set, 'cohort')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_set contains cohort 2018\n",
    "val_set = pd.read_csv('../../data/val_set.csv')\n",
    "fun.to_obj(val_set, 'cohort')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set contrains cohort 2019\n",
    "test_set = pd.read_csv('../../data/test_set.csv')\n",
    "fun.to_obj(test_set, 'cohort')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then took a look at the numeric features of the Train Set to check the correlations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = train_set.select_dtypes(['float64', 'int64'])\n",
    "\n",
    "fun.heatmap(num_features, 'high_school_4_year_graduation_rate_total')\n",
    "plt.savefig('../../reports/figures/correlations_map.png')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This heatmap shows the postive and negative correlations to the target variable.\n",
    "\n",
    "Not surprisingly, features such as 5-year graduation rate, student attendance rate, and teacher retention rate have a strong positive correlation to 4-year graduation. While high school dropout rate and chronic truancy have a strong negative correlation.\n",
    "\n",
    "**A note about 4-year vs 5-year graduation**: A student who completes all graduation requirements in the traditional 4-year schedule is included in the 4-year graduation rate total. If a student takes an extra year to complete all graduation requirements they are included in the 5-year graduation rate total. They may graduate in the same year, but are counted separately in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Simple Model\n",
    "\n",
    "I used a basic OLS Linear Regression to set a base line using [this forward selected function](#https://planspace.org/20150423-forward_selection_with_statsmodels/) to select the numeric features.\n",
    "   \n",
    "  >\"It tries to optimize adjusted R-squared by adding features that help the most one at a time until the score goes down or you run out of features.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fsm = fun.forward_selected(num_features, 'high_school_4_year_graduation_rate_total')\n",
    "fsm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model returned an R-squared value of 0.445, indicating that less than 50% of the the variance in the target variable is predictable from the features. This was the baseline model, did not include any of the categorical features, and there was no feature engineering or hyperparameter tuning. That leaves me feeling optimistic that I can improve on this with all of the above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering and selection\n",
    "\n",
    "The next steps are to separate the target from the features in my three sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_set.drop('high_school_4_year_graduation_rate_total', axis=1)\n",
    "y_train = train_set.high_school_4_year_graduation_rate_total\n",
    "\n",
    "X_val = val_set.drop('high_school_4_year_graduation_rate_total', axis=1)\n",
    "y_val = val_set.high_school_4_year_graduation_rate_total\n",
    "\n",
    "X_test = test_set.drop('high_school_4_year_graduation_rate_total', axis=1)\n",
    "y_test = test_set.high_school_4_year_graduation_rate_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Impute and Scale the numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify numeric features\n",
    "train_num = X_train.select_dtypes(['float64', 'int64'])\n",
    "val_num = X_val.select_dtypes(['float64', 'int64'])\n",
    "test_num = X_test.select_dtypes(['float64', 'int64'])\n",
    "# instatiate imputer\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "# fit on the training features\n",
    "imputer.fit(train_num)\n",
    "\n",
    "# transform training, validation, and testing data\n",
    "train_imp= imputer.transform(train_num)\n",
    "val_imp = imputer.transform(val_num)\n",
    "test_imp = imputer.transform(test_num)\n",
    "\n",
    "# return to DataFrames\n",
    "X_train_imp = pd.DataFrame(train_imp, columns=train_num.columns, index=X_train.index)\n",
    "X_val_imp = pd.DataFrame(val_imp, columns=val_num.columns, index=X_val.index)\n",
    "X_test_imp = pd.DataFrame(test_imp, columns=test_num.columns, index=X_test.index)\n",
    "\n",
    "# identify features\n",
    "X_train_num = X_train_imp.select_dtypes(['float64', 'int64'])\n",
    "X_val_num = X_val_imp.select_dtypes(['float64', 'int64'])\n",
    "X_test_num = X_test_imp.select_dtypes(['float64', 'int64'])\n",
    "\n",
    "# instatiate scaler\n",
    "ss = StandardScaler()\n",
    "\n",
    "# train on the training features\n",
    "ss.fit(X_train_num)\n",
    "\n",
    "# transform training, validation, and testing data\n",
    "X_train_sc= ss.transform(X_train_num)\n",
    "X_val_sc = ss.transform(X_val_num)\n",
    "X_test_sc = ss.transform(X_test_num)\n",
    "\n",
    "# return to DataFrame\n",
    "X_train_sc = pd.DataFrame(X_train_sc, columns=X_train_num.columns, index=X_train.index)\n",
    "X_val_sc = pd.DataFrame(X_val_sc, columns=X_val_num.columns, index=X_val.index)\n",
    "X_test_sc = pd.DataFrame(X_test_sc, columns=X_test_num.columns, index=X_test.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Select and One Hot Encode categorical features.\n",
    "I selected district_type, district_size, and school_type.\n",
    "Features such as city, disctrict, county, etc had so many values that it would have created an unmanageable number of encoded features without adding much return. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify features\n",
    "X_train_obj = X_train.select_dtypes(['object']).astype('category')\n",
    "X_val_obj = X_val.select_dtypes(['object']).astype('category')\n",
    "X_test_obj = X_test.select_dtypes(['object']).astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There was one last bit of data cleaning to perform, as I discovered some cells had white space of various lengths that needed to be stripped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip white space from strings\n",
    "X_train_cat = X_train_obj.loc[:, ('district_type', 'district_size', 'school_type')]\n",
    "X_train_cat.district_type = X_train_obj.district_type.str.rstrip()\n",
    "X_train_cat.district_size = X_train_obj.district_size.str.rstrip()\n",
    "X_train_cat.school_type = X_train_cat.school_type.str.rstrip()\n",
    "\n",
    "X_val_cat = X_val_obj.loc[:, ('district_type', 'district_size', 'school_type')]\n",
    "X_val_cat.district_type = X_train_obj.district_type.str.rstrip()\n",
    "X_val_cat.district_size = X_train_obj.district_size.str.rstrip()\n",
    "X_val_cat.school_type = X_train_cat.school_type.str.rstrip()\n",
    "\n",
    "X_test_cat = X_test_obj.loc[:, ('district_type', 'district_size', 'school_type')]\n",
    "X_test_cat.district_type = X_train_obj.district_type.str.rstrip()\n",
    "X_test_cat.district_size = X_train_obj.district_size.str.rstrip()\n",
    "X_test_cat.school_type = X_train_cat.school_type.str.rstrip()\n",
    "\n",
    "#encode the categoricals\n",
    "ohe = OneHotEncoder(drop='first')\n",
    "\n",
    "# fit on training data\n",
    "ohe.fit(X_train_cat)\n",
    "\n",
    "# transform training, validation, and testing data\n",
    "X_train_ohe = ohe.transform(X_train_cat).toarray()\n",
    "X_val_ohe = ohe.transform(X_val_cat).toarray()\n",
    "X_test_ohe = ohe.transform(X_test_cat).toarray()\n",
    "\n",
    "# return to DataFrames\n",
    "X_train_processed = pd.DataFrame(X_train_ohe, columns=ohe.get_feature_names(X_train_cat.columns))\n",
    "X_val_processed = pd.DataFrame(X_val_ohe, columns=ohe.get_feature_names(X_val_cat.columns))\n",
    "X_test_processed = pd.DataFrame(X_test_ohe, columns=ohe.get_feature_names(X_test_cat.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the numerical and categorical features back together\n",
    "X_train_all = pd.concat([X_train_sc, X_train_processed], axis=1)\n",
    "X_val_all = pd.concat([X_val_sc, X_val_processed], axis=1)\n",
    "X_test_all = pd.concat([X_test_sc, X_test_processed], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection\n",
    "For model selection, I decided to compare several models on their default settings and the same random state (19).\n",
    "\n",
    "The models I chose are \n",
    "- Linear Regression\n",
    "- Random Forest Regressor\n",
    "- Extra Trees Regressor\n",
    "- Lasso\n",
    "- Ridge\n",
    "- Gradient Boosting Regressor\n",
    "- Support Vector Regressor\n",
    "- K-Nearest Neighbor Regressor\n",
    "\n",
    "The results are plotted below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model_compare.png](../../reports/figures/model_compare.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting returned the highest R-squared value: 0.557.\n",
    "\n",
    "I then employed RandomizedSearchCV to determine the optimal settings for the hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function to be optimized\n",
    "loss = ['ls', 'lad', 'huber']\n",
    "\n",
    "# Number of trees used in the boosting process\n",
    "n_estimators = [100, 500, 900, 1100, 1500]\n",
    "\n",
    "# Maximum depth of each tree\n",
    "max_depth = [2, 3, 5, 10, 15]\n",
    "\n",
    "# Minimum number of samples per leaf\n",
    "min_samples_leaf = [1, 2, 4, 6, 8]\n",
    "\n",
    "# Minimum number of samples to split a node\n",
    "min_samples_split = [2, 4, 6, 10]\n",
    "\n",
    "# Maximum number of features to consider for making splits\n",
    "max_features = ['auto', 'sqrt', 'log2', None]\n",
    "\n",
    "# Define the grid of hyperparameters to search\n",
    "hyperparameter_grid = {'n_estimators': n_estimators,\n",
    "                       'max_depth': max_depth,\n",
    "                       'min_samples_leaf': min_samples_leaf,\n",
    "                       'min_samples_split': min_samples_split,\n",
    "                       'max_features': max_features}\n",
    "\n",
    "# Create the model to use for hyperparameter tuning\n",
    "model_1 = GradientBoostingRegressor(random_state = 19)\n",
    "\n",
    "# Set up the random search with 4-fold cross validation\n",
    "random_cv_1 = RandomizedSearchCV(estimator=model_1,\n",
    "                               param_distributions=hyperparameter_grid,\n",
    "                               cv=4, n_iter=25, \n",
    "                               scoring = 'r2',\n",
    "                               n_jobs = -1, verbose = 1, \n",
    "                               return_train_score = True,\n",
    "                               random_state=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit on the training data\n",
    "random_cv_1.fit(X_train_all, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best combination of settings\n",
    "random_cv_1.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then fit and evaluated the \"best\" model on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model_1 = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
    "                          learning_rate=0.1, loss='ls', max_depth=3,\n",
    "                          max_features='log2', max_leaf_nodes=None,\n",
    "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                          min_samples_leaf=6, min_samples_split=2,\n",
    "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "                          n_iter_no_change=None, presort='auto',\n",
    "                          random_state=19, subsample=1.0, tol=0.0001,\n",
    "                          validation_fraction=0.1, verbose=0, warm_start=False)\n",
    "\n",
    "\n",
    "# train the model\n",
    "model_1.fit(X_train_all, y_train)\n",
    "    \n",
    "# make predictions and evalute\n",
    "model_1_pred = model_1.predict(X_val_all)\n",
    "model_1_r2 = r2_score(y_val, model_1_pred)\n",
    "\n",
    "print('Model_1 Performance on the validation set: R2 = %0.3f' % model_1_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The model returned an R-squared value of 0.589.\n",
    "This was a slight improvement over the model's score of 0.557 in its' default state.\n",
    "While some further manual tuning of hyperparameters may offer some additional improvement to the score, I don't "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best model\n",
    "final_model = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
    "                          learning_rate=0.1, loss='ls', max_depth=3,\n",
    "                          max_features='log2', max_leaf_nodes=None,\n",
    "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                          min_samples_leaf=6, min_samples_split=2,\n",
    "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "                          n_iter_no_change=None, presort='auto',\n",
    "                          random_state=19, subsample=1.0, tol=0.0001,\n",
    "                          validation_fraction=0.1, verbose=0, warm_start=False)\n",
    "\n",
    "# Make predictions on the test set using final model\n",
    "final_model.fit(X_train_all, y_train)\n",
    "final_pred = final_model.predict(X_test_all)\n",
    "final_r2 = r2_score(y_test, final_pred)\n",
    "\n",
    "print('Final Model Performance on the test set: R2 = %0.3f' % final_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model Evaluation\n",
    "#### The final model returned an R-squared value of 0.684.\n",
    "That is a surprisingly good result on the test data. The R-squared values have been steadily increasing throughout is process, as seen in the chart below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model                                         | Evaluated on   | R-Squared Score |\n",
    "|-----------------------------------------------|----------------|-----------------|\n",
    "| Ordinary Least Squares  Linear Regression     | Validation set | 0.445           |\n",
    "| Gradient Boosting Regressor (default setting)   | Validation set | 0.557           |\n",
    "| Gradient Boosting Regressor (optimized setting) | Validation set | 0.589           |\n",
    "| Gradient Boosting Regressor (optimized setting) | Test set       | 0.684           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the First Simple Model to the Final Model there was an increase of approximately 24 percentage points.\n",
    "This indicates that the model is improving at each iteration.\n",
    "Although the R-squared values are increasing, there are no giant leaps that suggest overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_results = pd.DataFrame({'feature': list(X_train_all.columns), \n",
    "                                'importance': final_model.feature_importances_})\n",
    "\n",
    "# Show the top 10 most important\n",
    "feature_results = feature_results.sort_values('importance', ascending = False).reset_index(drop=True)\n",
    "\n",
    "feature_results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon reviewing the feature importances of the final model, I notice that it is very similar to the correlation heatmap made during EDA.\n",
    "The features with the highest positive and negative correlations were also the highest on this ranking of importance.\n",
    "\n",
    "I also notice that none of the categorical features were in the top 10 on this list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "At this point, I feel this result is a \"proof of concept\" for the project. The R-squared value of 0.684 on the final model does not satisfy me that it could be used for predictions in the real world at this time. However, there is still the possibility of building upon this start and attempting to improve its' ability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Improvement Ideas\n",
    "\n",
    "Some ideas I would like to pursue include:\n",
    "- optimizing some of the other models I tried during selection.\n",
    "Gradient Boosting returned the highest value in the default state, but Ridge, KNN, Lasso, and Extra Trees were all close behind.\n",
    "\n",
    "- additional feature engineering and feature selection\n",
    "I would like to reassess some of the features that I eliminated in EDA.\n",
    "Perhaps it would be possible to find more about the missing values, include some of the data recorded at the district level, and possibly look at including financial, tax, or census information.\n",
    "\n",
    "- new code for data cleaning\n",
    "As a side note to the model building: If I join more data sources, I would like to work on some code to gather and compile data more elegantly and efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contact\n",
    "\n",
    "#### Dann Morr [GitHub](https://github.com/dannmorr) | [LinkedIn](https://linkedin.com/in/dannmorr) | [Medium](https://medium.com/@dannmorr) | [Email](mailto:dannmorr@gmail.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presentation Slide Deck\n",
    "[View here](https://github.com/dannmorr/predicting-ibse-graduation-rate/blob/master/reports/presentation.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grad-env",
   "language": "python",
   "name": "grad-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
